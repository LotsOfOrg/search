{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682351e-0abb-4227-967d-593095f75f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp index\n",
    "\n",
    "from search.core import *\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import Set, DefaultDict\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266b30b-8a06-4ace-a8e4-512a2dd29ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InvertedIndex(Index):\n",
    "    \"Basic inverted index implementation mapping terms to document IDs\"\n",
    "    def __init__(self):\n",
    "        self.index: DefaultDict[str, Set[str]] = defaultdict(set)  # term -> doc_ids\n",
    "        self.documents: Dict[str, Document] = {}  # doc_id -> Document\n",
    "        \n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        \"Basic tokenization - we'll improve this later with proper tokenization\"\n",
    "        return [w.lower() for w in re.findall(r'\\w+', text)]\n",
    "    \n",
    "    def add(self, doc: Document):\n",
    "        \"Add a document to the index\"\n",
    "        self.documents[doc.id] = doc\n",
    "        terms = self._tokenize(doc.content)\n",
    "        for term in terms:\n",
    "            self.index[term].add(doc.id)\n",
    "            \n",
    "    def remove(self, doc_id: str):\n",
    "        \"Remove a document from the index\"\n",
    "        if doc_id not in self.documents: return\n",
    "        doc = self.documents[doc_id]\n",
    "        terms = self._tokenize(doc.content)\n",
    "        for term in terms:\n",
    "            self.index[term].discard(doc_id)\n",
    "        del self.documents[doc_id]\n",
    "    \n",
    "    def search(self, query: Query) -> list[SearchResult]:\n",
    "        \"Search using TF-IDF scoring\"\n",
    "        terms = self._tokenize(query.text)\n",
    "        if not terms: return []\n",
    "        \n",
    "        # Get matching doc_ids\n",
    "        matching_ids = set.intersection(*(self.index[term] for term in terms))\n",
    "        \n",
    "        # Score matches (simple term frequency for now)\n",
    "        results = []\n",
    "        for doc_id in matching_ids:\n",
    "            doc = self.documents[doc_id]\n",
    "            doc_terms = self._tokenize(doc.content)\n",
    "            score = sum(doc_terms.count(term) for term in terms) / len(doc_terms)\n",
    "            results.append(SearchResult(doc, score))\n",
    "        \n",
    "        return sorted(results, key=lambda x: x.score, reverse=True)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"Clear all documents from the index\"\n",
    "        self.index.clear()\n",
    "        self.documents.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c735fba-3022-46e3-93be-9c425b448779",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = InvertedIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a8bae-071c-4930-8011-fbfcf7a59630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test adding and searching\n",
    "doc1 = Document(\"1\", \"The quick brown fox jumps over the lazy dog\")\n",
    "doc2 = Document(\"2\", \"The lazy fox sleeps\")\n",
    "idx.add(doc1)\n",
    "idx.add(doc2)\n",
    "\n",
    "results = idx.search(Query(\"fox\"))\n",
    "test_eq(len(results), 2)\n",
    "test_eq({r.document.id for r in results}, {\"1\", \"2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c04fc4-2d38-4f77-9735-8bd0384fc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test removal\n",
    "idx.remove(\"1\")\n",
    "results = idx.search(Query(\"fox\"))\n",
    "test_eq(len(results), 1)\n",
    "test_eq(results[0].document.id, \"2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
