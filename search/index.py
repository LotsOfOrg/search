# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_indexes.ipynb.

# %% auto 0
__all__ = ['InvertedIndex']

# %% ../nbs/01_indexes.ipynb 1
class InvertedIndex(Index):
    "Basic inverted index implementation mapping terms to document IDs"
    def __init__(self):
        self.index: DefaultDict[str, Set[str]] = defaultdict(set)  # term -> doc_ids
        self.documents: Dict[str, Document] = {}  # doc_id -> Document
        
    def _tokenize(self, text: str) -> list[str]:
        "Basic tokenization - we'll improve this later with proper tokenization"
        return [w.lower() for w in re.findall(r'\w+', text)]
    
    def add(self, doc: Document):
        "Add a document to the index"
        self.documents[doc.id] = doc
        terms = self._tokenize(doc.content)
        for term in terms:
            self.index[term].add(doc.id)
            
    def remove(self, doc_id: str):
        "Remove a document from the index"
        if doc_id not in self.documents: return
        doc = self.documents[doc_id]
        terms = self._tokenize(doc.content)
        for term in terms:
            self.index[term].discard(doc_id)
        del self.documents[doc_id]
    
    def search(self, query: Query) -> list[SearchResult]:
        "Search using TF-IDF scoring"
        terms = self._tokenize(query.text)
        if not terms: return []
        
        # Get matching doc_ids
        matching_ids = set.intersection(*(self.index[term] for term in terms))
        
        # Score matches (simple term frequency for now)
        results = []
        for doc_id in matching_ids:
            doc = self.documents[doc_id]
            doc_terms = self._tokenize(doc.content)
            score = sum(doc_terms.count(term) for term in terms) / len(doc_terms)
            results.append(SearchResult(doc, score))
        
        return sorted(results, key=lambda x: x.score, reverse=True)
    
    def clear(self):
        "Clear all documents from the index"
        self.index.clear()
        self.documents.clear()
